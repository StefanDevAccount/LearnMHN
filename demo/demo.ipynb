{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `mhn` Demonstration\n",
    "\n",
    "In this notebook, we will demonstrate a basic workflow for using the `mhn` python package. As exemplary data, we use somatic mutations in 12 driver genes of lung cancer patients. A similar analysis has been conducted on the same dataset in the publication __[Overcoming Observation Bias for Cancer Progression Modeling](https://www.biorxiv.org/content/10.1101/2023.12.03.569824v1)__ and was originally obtained through __[AACR GENIE](https://doi.org/10.7303/syn51355584)__. \n",
    "\n",
    "In the following sections, we will\n",
    "1. [Check the necessary installations](#confirming-package-functionality)\n",
    "2. [Load and inspect input data](#loading-and-inspecting-input-data)\n",
    "3. [Prepare for MHN training](#preparation-for-mhn-training)\n",
    "    1. [Choose a framework](#choosing-the-mhn-framework)\n",
    "    2. [Choose a penalty](#choosing-the-regularization-penalty)\n",
    "    3. [Prepare for cross-validation](#choosing-the-regularization-penalty)\n",
    "    4. [Choose a processing device](#choosing-the-processing-device)\n",
    "4. [Execute MHN training with cross-validation](#executing-mhn-training)\n",
    "    1. [Perform cross-validation](#cross-validation)\n",
    "    2. [Train the final models](#final-model-training)\n",
    "5. [Inspect the primary MHN output - the Θ-matrix](#inspecting-the-θ-matrix)\n",
    "6. [Simulate artifical data from the trained models...](#simulating-artificial-data-from-a-mhn)\n",
    "    1. [... to evaluate fit quality](#evaluating-mhn-fitting-quality)\n",
    "    2. [... to explore event orderings](#exploring-event-orderings)\n",
    "7. [Make next-event predictions using MHN inferences](#predicting-the-next-event-for-a-given-mhn-state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirming package functionality\n",
    "\n",
    "This notebook assumes that the package has been installed via `pip install mhn`. We can confirm the functionality of the installation with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import mhn\n",
    "    print(\"mhn is installed\")\n",
    "except ImportError:\n",
    "    print(\"mhn installation not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make all MHN imports necessary for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mhn.optimizers import Optimizer, MHNType, Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If available, a Nvidia GPU may be used to accelerate MHN training. Apart from the GPU itself, this requires a functional installation of __[CUDA and the CUDA compiler nvcc](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/)__. This approach is optional and will be treated as such in the notebook. \n",
    "We can check if CUDA is available to MHN with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_status = mhn.cuda_available()\n",
    "print(cuda_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting and loading input data\n",
    "\n",
    "We now import the lung adenocarcinoma dataset. MHN expects a binary matrix (containing only 0s and 1s), with rows representing individual observations (e.g., patients/tumors), columns representing individual events (e.g., per-gene mutations) and cells representing the presence or absence (1s or 0s) of the respective event in the respective observation. Event identifiers may be given as column names. To inspect the input data, we can load it with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input = pd.read_csv('LUAD_n12.csv')\n",
    "input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now inspect some basic data stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of observations:\", len(input))\n",
    "print(\"Number of events:\", len(input.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Event frequencies:\")\n",
    "input.sum(axis=0) / len(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distribution of active event counts across observations:\")\n",
    "input.sum(axis=1).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is a little large to be used completely in this notebook, so let's only take a part of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(6)\n",
    "input_subset = input.sample(n=500)\n",
    "input_subset.sum(axis=1).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for MHN training\n",
    "\n",
    "In preparation for MHN training, one has to make a few decisions regarding the model framework and cross-validation strategies. We will now guide through these decisions step by step. Afterwards, the actual training process, including cross-validation, will be two simple lines of code.\n",
    "\n",
    "### Choosing the MHN framework\n",
    "One can use either the classical MHN framework (**cMHN**, discussed in __[Schill et al. 2020](https://doi.org/10.1093/bioinformatics/btz513)__) or the observation-aware MHN framework (**oMHN**, discussed in __[Schill et al. 2024](https://www.biorxiv.org/content/10.1101/2023.12.03.569824v1)__).<br><br> In brief, the conceptual difference is that oMHN explicitly includes the observation itself as an event. In other words, the time at which a cancer exits (recorded) progression by being observed is dependent on the events that it accumulated before. In contrast, in cMHN this dependence does not exist and instead observation times simply follow an exponential distribution. In practice, choosing oMHN generally does not substantially affect performance in terms of fitting the data, but it likely increases soundness and interpretability of the inferred model parameters. **Thus, we would recommend oMHN in most scenarios.** <br><br>\n",
    "In terms of code, this decision is made by setting the respective optimizer function. For the example at hand, we will perform a run with each framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cMHN, we set\n",
    "cMHN_opt = Optimizer(mhn_type=MHNType.cMHN)\n",
    "# while for oMHN, we set\n",
    "oMHN_opt = Optimizer(mhn_type=MHNType.oMHN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have initialised the optimizer object, we can give it the input dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data for the cMHN framework\n",
    "cMHN_opt.load_data_matrix(input_subset)\n",
    "# load data for the oMHN framework\n",
    "oMHN_opt.load_data_matrix(input_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could have used the `load_data_from_csv()` function to load the input data directly from a .csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the regularization penalty\n",
    "\n",
    "Like most machine learning models, MHN uses regularization during the training process. In brief, regularization means penalising the parameter inference so that a sparse (parsimonious) set of parameters is generally preferred. In other words, it aims at explaining the training input data well, with as little parameters as possible, to keep the model generalisable to test data. While the *strength* of the regularization penalty is determined later during cross-validation, we now have to decide on the *form* of regularization. <br><br>\n",
    "There are two forms of penalty currently implemented for MHN, a **standard L1-penalty** and a **custom symmetrical penalty**. The L1-penalty is a standard approach in machine learning and keeps each parameter uniformly close to 0. In contrast, the symmetrical penalty restricts pairs of parameters simultaneously, specifically pairs which are symmetrical in Θ with respect to the diagonal. Having one such pair of parameters at a nonzero value is as costly as having only one of them at the same value. This is further discussed in __[Schill et al. 2024](https://www.biorxiv.org/content/10.1101/2023.12.03.569824v1)__. **We recommend to choose the penalty based on prior knowledge about the structure of effects that are expected in the type of data one is working with. For mutational data as in this notebook, we recommend the symmetrical penalty as it is likely more in line with the nature of epistatic effects between driver mutations.**<br><br>\n",
    "In terms of code, this decision is made by calling the respective penalty-setting function. In our example, we set the L1-penalty for the cMHN run and the symmetrical penalty for the oMHN run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the L1-penalty, we set\n",
    "cMHN_opt.set_penalty(cMHN_opt.Penalty.L1)\n",
    "# while for the symmetrical penalty, we set\n",
    "oMHN_opt.set_penalty(oMHN_opt.Penalty.SYM_SPARSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing cross-validation hyperparameters\n",
    "\n",
    "In cross-validation, a range of different regularization penalty strengths (\"lambdas\") will be tested to identify one which yields optimal performance on holdout data. Here, we have to decide on some hyperparameters:\n",
    "- The minimum and maximum penalty strength (\"lambda\") to be tested\n",
    "- The amount of steps (i.e., length of the lambda sequence) to take between these limits\n",
    "- The amount of folds we want to split our data into for cross-validation\n",
    "\n",
    "Generally, a large range with small step size and many folds will yield the most accurate results while increasing computation time. For the purpose of this demonstration however, we will pick values that are easy on the computation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, one can expect that 1 divided by the number of observations will yield a decent cross-validation result, so we can design our minimum and maximum values around it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_min = 0.1/len(input_subset)\n",
    "lambda_max = 100/len(input_subset)\n",
    "\n",
    "print(\"Minimum lambda:\", lambda_min)\n",
    "print(\"Maximum lambda:\", lambda_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more steps in between min/max, the more accuate the cross-validation will be within the given limits. However, here we pick a small amount to keep computation time low:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cv_steps = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When provided with min/max values and a number of steps, MHN's cross-validation will run with the respective number of intermediate lambda values, spaced equidistantly on a log scale. Alternatively, one specify all values, including intermediate steps. For the purpose of this demonstration, we will explicitely specify the full range albeit using default MHN behavior: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lambda_sequence = np.exp(np.linspace(\n",
    "    np.log(lambda_min + 1e-10), np.log(lambda_max + 1e-10), n_cv_steps))\n",
    "\n",
    "print(\"Lambda sequence:\")\n",
    "lambda_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we have to define the number of cross-validation folds. For this demonstration, we will just do 3 folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cv_folds = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the processing device\n",
    "\n",
    "As the last step before training, we have to make a purely technical decision: If our device has a Nvidia GPU and CUDA is installed, we can opt to perform (some of) the intensive computing on the GPU. This may yield significant speedup, especially of our input matrix is densely populated with 1s. If our device does not have this capability, we can simply use the CPU instead. The default behavior of MHN is to use the GPU only for parts of the data where it is likely beneficial. <br><br>\n",
    "\n",
    "To ensure accessibility, we will opt for CPU-only in this demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses both CPU and GPU depending on the number of mutations in the individual sample (default)\n",
    "# cMHN_opt.set_device(Device.AUTO)\n",
    "\n",
    "# use the GPU to compute log-likelihood score and gradient\n",
    "# cMHN_opt.set_device(Device.GPU)\n",
    "\n",
    "# use the CPU to compute log-likelihood score and gradient\n",
    "cMHN_opt.set_device(Device.CPU)\n",
    "\n",
    "# and we will do the same for the oMHN run:\n",
    "oMHN_opt.set_device(Device.CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing MHN training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "Once our preparation is done, we can call the `lambda_from_cv()` function to perform the cross-validation, providing the previously set parameters. To reiterate, we have the option of either providing an explicit sequence of lambdas or defining min, max and a number of steps. Furthermore, we specify the number of cross-validation folds and whether we want a progress tracker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validate cMHN\n",
    "cMHN_lambda = cMHN_opt.lambda_from_cv(\n",
    "    lambda_min=lambda_min, lambda_max=lambda_max, steps=n_cv_steps, nfolds=n_cv_folds, show_progressbar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oMHN_lambda = oMHN_opt.lambda_from_cv(\n",
    "    lambda_min=lambda_min, lambda_max=lambda_max, steps=n_cv_steps, nfolds=n_cv_folds, show_progressbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When cross-validation has finished, let's see where in the given sequence our optimal lambdas are located (having them at the extremes may indicate that the sequence needs to be adjusted):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot original lambda sequence\n",
    "plt.scatter(lambda_sequence, np.full(len(lambda_sequence), 1), color='black')\n",
    "\n",
    "# plot cross-validated lambdas\n",
    "plt.scatter(cMHN_lambda, 2, color='red')\n",
    "plt.scatter(oMHN_lambda, 3, color='blue')\n",
    "\n",
    "# labels\n",
    "plt.text(cMHN_lambda, 1.9, \"cMHN\", ha='center')\n",
    "plt.text(oMHN_lambda, 2.9, \"oMHN\", ha='center')\n",
    "\n",
    "# log scale\n",
    "plt.xscale('log')\n",
    "plt.yticks([])\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the final models, using the optimal lambdas from cross-validation we have just confirmed. Then, we'll save the main output, the Θ-matrix, as .csv file, alongside a log file in .json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train cMHN\n",
    "cMHN_opt.train(lam=cMHN_lambda)\n",
    "# save output\n",
    "cMHN_opt.result.save(filename=\"cMHN.csv\")\n",
    "\n",
    "# train oMHN\n",
    "oMHN_opt.train(lam=oMHN_lambda)\n",
    "# save output\n",
    "oMHN_opt.result.save(filename=\"oMHN.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the outputs. First, let's look at the log files which tell us about important technical details of the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "\n",
    "with open('cMHN_meta.json') as f:\n",
    "    cMHN_log = json.load(f)\n",
    "\n",
    "pprint.pprint(cMHN_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through these one by one:\n",
    "- `'init'` tells us about the initial state of Θ at the start of training. By default (`'init': None`), MHN initializes training with the \"independence model\" - in which the diagonal entries of Θ are set to the empirical odds and the rest of Θ is empty. Alternatively, users have the option to specify some prior parameterization of Θ using the `set_init_theta()` function.\n",
    "- `'lambda'` shows us the input regularization strength we previously determined through CV.\n",
    "- `'maxit'` is the specified maximum number of gradient steps before the training is automatically terminated (default: 5000). \n",
    "- `'message'` - is the termination message obtained internally from `scipy.optimize.minimize()`. `CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH` is the expected value, which means that the optimization terminated successfully due to parameter convergence. In case other values pop up here, the [scipy documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) should be consulted. \n",
    "- `'nit'` tells us about the number of gradient steps taken before termination.\n",
    "- `'reltol'` informs us about the input upper bound for parameter changes that are still considered as converged, see the [scipy documentation](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-bfgs.html#optimize-minimize-bfgs). This parameter can be changed from default (`1e-07`) in the `train()` function but we advise inexperienced users not to do so. \n",
    "- `'score'` informs us about the input data likelihood under the final model. Specifically, it shows the negative logarithm of the sum of likelihoods over all observations (to be minimized during training - the lower, the better). \n",
    "- `'status'` is the termination status of the optimizer. Refer to the [documentation of the scipy `OptimizeResult`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html#scipy.optimize.OptimizeResult) for more information.\n",
    "<br><br>\n",
    "Now, let's look at the same output for oMHN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('oMHN_meta.json') as f:\n",
    "    oMHN_log = json.load(f)\n",
    "\n",
    "pprint.pprint(oMHN_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see e.g., that the oMHN run needed a few more steps to converge and achieved a slightly better score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Θ-matrix\n",
    "\n",
    "To visualise the main MHN output which details all inferred parameters (i.e., event accumulation rates and rate changes between events), we can simply call `plot()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cMHN_opt.result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `plot()` visualizes the logarithmised form of Θ - this can be changed with `logarithmic = False`. The logarithmic version is also written to file by `save()`. <br><br>\n",
    "\n",
    "The base rates of the events are shown on the left in green.\n",
    "\n",
    "The matrix on the right in blue and green shows effects between events:\n",
    "A cell in row x and column y shows the (logarithmized) multiplicative effect that presence of event y has on the rate of event x. For example, presence of STK11 increases the rate of KEAP1 to `e^2.24 = 9.39 = 939%`. Conversely, presence of BRAF decreases the rate of KRAS to `e^-0.83 = 0.44 = 44%`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oMHN_opt.result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With oMHNs, we have an additional bottom row showing the effects on the observation event. For example, presence of TP53 increases the rate of observing the given cancer to `e^0.74 = 2.10 = 210%`. \n",
    "<br><br>\n",
    "Comparing the two matrices, they look quite different at the first glance. Some differences are due to choice of penalty form (L1 vs symmetrical) and some others are due to choice of framework (cMHN vs oMHN). For instance, we can see that the positive relationship between STK11 and KEAP1 is explained by a unidirectional, strong effect with the L1 penalty while it is explained by symmetric, bidirectional and more moderate effects with the symmetrical penalty. In general, the symmetrical-penalty-trained oMHN is (expectedly) much more symmetric.<br> <br>\n",
    "Furthermore, we can see that the cMHN heatmap has many more negative effects. This is likely because there is a lot of statistical mutual exclusivity in the data that needs to be explained by cMHN. For instance, EGFR is often observed alone or just with 1-2 co-mutations. cMHN can explain this pattern only by introducing negative rate changes between EGFR and other events. oMHN instead can alternatively explain this pattern by fitting an observation-rate-increasing effect to EGFR - in other words, EGFR is often lonely not because it suppresses other events, but because the respective tumor likely becomes observed before other events can accumulate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating artificial data from a MHN\n",
    "\n",
    "We now have an idea of the parameters that were inferred in training. Now, we can use them to generate artificial data. There are two main purposes for this:\n",
    "1. **Evaluating the fit** - comparing statistical pattern between real and simulated data can be useful to validate that MHN actually fits the data it was trained on.\n",
    "2. **Exploring event orderings** - we can simulate cancers while keeping track of the ordering in which events accumulate, giving us the possibility to answer related questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating MHN fitting quality\n",
    "Let's now first simulate some simple (final) observations without orderings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate 500 samples for cMHN\n",
    "cMHN_sim_observations = cMHN_opt.result.sample_artificial_data(\n",
    "    500, as_dataframe=True)\n",
    "print(\"cMHN simulations:\")\n",
    "cMHN_sim_observations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate 500 samples for oMHN\n",
    "oMHN_sim_observations = oMHN_opt.result.sample_artificial_data(\n",
    "    500, as_dataframe=True)\n",
    "print(\"oMHN simulations:\")\n",
    "oMHN_sim_observations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have simulated as many input observations, once for cMHN and oMHN each. Let's now conduct a simple Fisher's test to check if co-occurrence patterns in the input data were recreated. First, we perform the analysis on the original input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import fisher_exact\n",
    "\n",
    "# make the cross-table on input data\n",
    "input_crosstable = pd.crosstab(input_subset['STK11'], input_subset['KEAP1'])\n",
    "\n",
    "# print results\n",
    "print(\"Cross-table of STK11 and KEAP1 mutations in original data:\")\n",
    "print(input_crosstable)\n",
    "\n",
    "# perform Fisher's exact test\n",
    "odds_ratio, p_value = fisher_exact(input_crosstable)\n",
    "\n",
    "# print results\n",
    "print(\"Fisher's exact test:\")\n",
    "print(\"Odds Ratio:\", odds_ratio)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a significant enrichment in the co-occurrence of the two mutations. Now, let's see if we got similar patterns in the cMHN simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the cross-table on cMHN data\n",
    "cMHN_crosstable = pd.crosstab(\n",
    "    cMHN_sim_observations['STK11'], cMHN_sim_observations['KEAP1'])\n",
    "\n",
    "# print results\n",
    "print(\"Cross-table of STK11 and KEAP1 mutations in data simulated from cMHN:\")\n",
    "print(cMHN_crosstable)\n",
    "\n",
    "# perform Fisher's exact test\n",
    "odds_ratio, p_value = fisher_exact(cMHN_crosstable)\n",
    "\n",
    "# print results\n",
    "print(\"Fisher's exact test:\")\n",
    "print(\"Odds Ratio:\", odds_ratio)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we do the same for oMHN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the cross-table on oMHN data\n",
    "oMHN_crosstable = pd.crosstab(\n",
    "    oMHN_sim_observations['STK11'], oMHN_sim_observations['KEAP1'])\n",
    "\n",
    "# print results\n",
    "print(\"Cross-table of STK11 and KEAP1 mutations in data simulated from oMHN:\")\n",
    "print(oMHN_crosstable)\n",
    "\n",
    "# perform Fisher's exact test\n",
    "odds_ratio, p_value = fisher_exact(oMHN_crosstable)\n",
    "\n",
    "# print results\n",
    "print(\"Fisher's exact test:\")\n",
    "print(\"Odds Ratio:\", odds_ratio)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we can see that both simulations faithfully recreated the original significant enrichment. Also the absolute numbers shown in the simulated cross-tables resemble the original data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring event orderings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use the simulations to get novel insights from MHN that are not contained in the input data (but learned from it). Specifically, we can use MHN e.g., to assess trends in the orderings of events. The possible questions are manifold, but let's demonstrate here a simple one: \n",
    "- **What is the probability that event A preceded B, given that an observation (cancer) has accumulated both?**\n",
    "To answer this question, we simply\n",
    "1. Simulate many observations from a MHN while keeping track of orderings - here we need a lot now but fortunately the simulation is very fast\n",
    "2. Condition the output to those who have both A and B\n",
    "3. Under the condition, calculate the fraction of trajectories where A preceded B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate from cMHN\n",
    "c_trajectories, c_times = cMHN_opt.result.sample_trajectories(\n",
    "    100000, [], output_event_names=True)\n",
    "\n",
    "# condition on trajectories which contain KRAS and TP53\n",
    "indices = np.array([i for i, trajectory in enumerate(\n",
    "    c_trajectories) if 'KRAS' in trajectory and 'TP53' in trajectory])\n",
    "conditioned_trajectories = [c_trajectories[index] for index in indices]\n",
    "\n",
    "# of those, count in how many KRAS preceded TP53 and then get the fraction\n",
    "count_KRAS_first = sum(trajectory.index('KRAS') < trajectory.index(\n",
    "    'TP53') for trajectory in conditioned_trajectories)\n",
    "fraction_KRAS_first = count_KRAS_first / len(conditioned_trajectories)\n",
    "\n",
    "print(\"cMHN: simulation-derived P(KRAS before TP53 | KRAS, TP53 present):\",\n",
    "      fraction_KRAS_first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do the same for oMHN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate from cMHN\n",
    "o_trajectories, o_times = oMHN_opt.result.sample_trajectories(\n",
    "    100000, [], output_event_names=True)\n",
    "\n",
    "# condition on trajectories which contain KRAS and TP53\n",
    "indices = np.array([i for i, trajectory in enumerate(\n",
    "    o_trajectories) if 'KRAS' in trajectory and 'TP53' in trajectory])\n",
    "conditioned_trajectories = [o_trajectories[index] for index in indices]\n",
    "\n",
    "# of those, count in how many KRAS preceded TP53 and then get the fraction\n",
    "count_KRAS_first = sum(trajectory.index('KRAS') < trajectory.index(\n",
    "    'TP53') for trajectory in conditioned_trajectories)\n",
    "fraction_KRAS_first = count_KRAS_first / len(conditioned_trajectories)\n",
    "\n",
    "print(\"oMHN: simulation-derived P(KRAS before TP53 | KRAS, TP53 present):\",\n",
    "      fraction_KRAS_first)"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both models encode trajectories where either KRAS or TP53 can come first. However, the oMHN suggests that KRAS slightly tends to come first, while the cMHN leans the other way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the next event for a given MHN state\n",
    "\n",
    "In principle, MHNs are capable of estimating the future progression of any state represented by a given MHN. For instance, simulations with non-empty starting states could be used. Additionally, we have implemented a convenience function that for a given state, returns the probabilities for all possible next events. <br> <br>\n",
    "\n",
    "To predict the next event for a state that already has KRAS and STK11, we'll give the function an array which is all zeroes apart from the respective columns. Let's check which indices we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_subset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the second and fourth entry to be 1, all others be 0. The function can output a simple array, or a dataframe that also contains event names (controlled with `as_dataframe=True`). Further, we can treat the state as unobserved, so that observation itself is also a legitimate next event (controlled with `allow_observation=True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify state of interest\n",
    "state_of_interest = np.array(\n",
    "    [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=np.int32)\n",
    "\n",
    "# get the cMHN predictions\n",
    "cMHN_predictions = cMHN_opt.result.compute_next_event_probs(\n",
    "    state_of_interest, as_dataframe=True)\n",
    "\n",
    "# show\n",
    "print(\"cMHN predictions:\")\n",
    "print(cMHN_predictions)\n",
    "\n",
    "# get the cMHN predictions, this time with possible observation\n",
    "cMHN_predictions = cMHN_opt.result.compute_next_event_probs(\n",
    "    state_of_interest, as_dataframe=True, allow_observation=True)\n",
    "\n",
    "# show\n",
    "print(\"cMHN predictions w/ observation:\")\n",
    "print(cMHN_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it again for oMHN: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the oMHN predictions\n",
    "oMHN_predictions = oMHN_opt.result.compute_next_event_probs(\n",
    "    state_of_interest, as_dataframe=True)\n",
    "\n",
    "# show\n",
    "print(\"oMHN predictions:\")\n",
    "print(oMHN_predictions)\n",
    "\n",
    "# get the cMHN predictions, this time with possible observation\n",
    "oMHN_predictions = oMHN_opt.result.compute_next_event_probs(\n",
    "    state_of_interest, as_dataframe=True, allow_observation=True)\n",
    "\n",
    "# show\n",
    "print(\"oMHN predictions w/ observation:\")\n",
    "print(oMHN_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both models favor KEAP1 as possible next event, although it is generally quite rare. This is because both models fit a positive interaction between STK11 and KEAP1. However, cMHN emphasizes this influence whereas in oMHN the effect is more moderate and TP53 is the next most likely event (excluding observation). We can also see that both models agree on that EGFR is unlikely to follow, although it is generally quite frequent. This is because both models fit a negative interaction between KRAS and EGFR. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_mhn_package",
   "language": "python",
   "name": "test_mhn_package"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
